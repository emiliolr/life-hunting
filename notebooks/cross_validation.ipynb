{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2e5e600-3eaa-40bd-951d-12d2536e3d1c",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b741169d-0f2f-4a52-a6bf-d5797e76771c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import GroupKFold, KFold\n",
    "from sklearn.metrics import balanced_accuracy_score, recall_score, mean_absolute_error\n",
    "from sklearn.linear_model import ElasticNetCV, LogisticRegressionCV\n",
    "\n",
    "from verde import BlockKFold\n",
    "\n",
    "from pymer4 import Lmer\n",
    "from flaml import AutoML\n",
    "\n",
    "from utils import read_csv_non_utf\n",
    "from model_utils import HurdleModelEstimator, PymerModelWrapper\n",
    "from custom_metrics import balanced_accuracy_FLAML, mean_absolute_error_range\n",
    "from cross_validation import run_cross_val, save_cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75c1d710-5157-4aed-8dd7-3c1eb8c3c8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in general configuration\n",
    "with open('../config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Getting filepaths\n",
    "gdrive_fp = config['gdrive_path']\n",
    "LIFE_fp = config['LIFE_folder']\n",
    "dataset_fp = config['datasets_path']\n",
    "benitez_lopez2019 = config['indiv_data_paths']['benitez_lopez2019']\n",
    "\n",
    "data_path = os.path.join(gdrive_fp, LIFE_fp, dataset_fp, benitez_lopez2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88aa18b7-0ec3-49ab-9b30-34e601f4f33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in data as a pandas dataframe\n",
    "ben_lop2019 = read_csv_non_utf(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ed7046-5b60-4c47-8420-1b2f938acfd7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Functions to run cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9402661c-2067-455e-959e-c902d889625f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.linear_model import ElasticNet, LogisticRegression\n",
    "\n",
    "from utils import direct_train_test, get_zero_nonzero_datasets, test_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6f2c271-d591-4825-9c30-d81a1399a844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same hurdle models in memory? False\n",
      "Same zero model in memory? False\n",
      "Same nonzero model in memory? False\n",
      "\n",
      "Original has fitted coefficients\n",
      "Copy DOESN'T have fitted coefficients\n"
     ]
    }
   ],
   "source": [
    "# Checking out how to safely clone an sklearn model\n",
    "hm = HurdleModelEstimator(zero_model = LogisticRegression(C = 10.0), nonzero_model = ElasticNet())\n",
    "\n",
    "#  checking memory sameness: fit one model but not the other...\n",
    "pp_data = preprocess_data(ben_lop2019, include_indicators = False, standardize = True, log_trans_cont = False,\n",
    "                          polynomial_features = 0)\n",
    "hm.fit(pp_data)\n",
    "\n",
    "hm_clone = clone(hm)\n",
    "print('Same hurdle models in memory?', hm is hm_clone)\n",
    "print('Same zero model in memory?', hm.zero_model is hm_clone.zero_model)\n",
    "print('Same nonzero model in memory?', hm.nonzero_model is hm_clone.nonzero_model)\n",
    "print()\n",
    "\n",
    "try:\n",
    "    hm.zero_model.coef_\n",
    "    print('Original has fitted coefficients')\n",
    "except AttributeError:\n",
    "    print('Original DOESN\\'T have fitted coefficients')\n",
    "\n",
    "try:\n",
    "    hm_clone.zero_model.coef_\n",
    "    print('Copy has fitted coefficients')\n",
    "except AttributeError:\n",
    "    print('Copy DOESN\\'T have fitted coefficients')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "62c53096-a446-4e6d-987e-76f2391f815e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cross_val(model, data, block_type = None, num_folds = 5, group_col = None, spatial_spacing = 5,\n",
    "                  fit_args = None, pp_args = None, class_metrics = None, reg_metrics = None, verbose = True, \n",
    "                  random_state = 1693, sklearn_submodels = False, back_transform = True, direct = None, \n",
    "                  tune_hurdle_thresh = False):\n",
    "\n",
    "    \"\"\"\n",
    "    A function to run k-fold cross-validation over a given dataset and with a given model. Multiple\n",
    "    types of blocking are supported, including spatial- and group-blocking of folds. Multiple model types\n",
    "    are supported as well, i.e., two-stage hurdle and direct regression/classification models.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : sklearn-like\n",
    "    data : pandas.DataFrame\n",
    "    block_type : string\n",
    "    num_folds : integer\n",
    "    group_col : string\n",
    "    spatial_spacing : integer\n",
    "    fit_args : dictionary\n",
    "    pp_args : dictionary\n",
    "    class_metrics : dictionary\n",
    "    reg_metrics : dictionary\n",
    "    verbose : boolean\n",
    "    random_state : integer\n",
    "    sklearn_submodels : boolean\n",
    "    back_transform : boolean\n",
    "    direct : string\n",
    "    tune_hurdle_thresh : boolean\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    metric_dict : dictionary\n",
    "    \"\"\"\n",
    "\n",
    "    # Setting mutable defaults\n",
    "    assert (class_metrics is not None) or (reg_metrics is not None), 'Please provide at least one classification or regression metric.'\n",
    "\n",
    "    if class_metrics is None:\n",
    "        class_metrics = {'per_class' : {}, 'overall' : {}}\n",
    "    if reg_metrics is None:\n",
    "        reg_metrics = {}\n",
    "    if fit_args is None and direct is not None:\n",
    "        fit_args = {}\n",
    "    if pp_args is None:\n",
    "        pp_args = {}\n",
    "\n",
    "    # Establishing k-fold parameters\n",
    "    if block_type is None:\n",
    "        if verbose:\n",
    "            print('Using standard cross-validation')\n",
    "        groups = None\n",
    "        kfold = KFold(n_splits = num_folds, random_state = random_state, shuffle = True)\n",
    "    elif block_type == 'group':\n",
    "        assert group_col is not None, 'If using group-based blocking, a group column must be specified using \"group_col.\"'\n",
    "        \n",
    "        if verbose:\n",
    "            print(f'Using group blocking on column {group_col}')\n",
    "        groups = data[group_col].values\n",
    "        kfold = GroupKFold(n_splits = num_folds)\n",
    "    elif block_type == 'spatial':\n",
    "        if verbose:\n",
    "            print(f'Using spatial blocking on with spacing {spatial_spacing} degrees')\n",
    "        groups = None\n",
    "        kfold = BlockKFold(spacing = spatial_spacing, n_splits = num_folds, shuffle = True, random_state = random_state)\n",
    "    \n",
    "    # Data structures for saving results\n",
    "    classes = {0 : 'low', 1 : 'medium', 2 : 'high'}\n",
    "    metric_dict = {}\n",
    "\n",
    "    for m in class_metrics['per_class']:\n",
    "        metric_dict[m] = {classes[c] : [] for c in classes}\n",
    "    for m in class_metrics['overall']:\n",
    "        metric_dict[m] = []\n",
    "    for m in reg_metrics:\n",
    "        metric_dict[m] = []\n",
    "\n",
    "    # Running the k-fold cross-validation\n",
    "    coords = data[['X', 'Y']].values\n",
    "    for i, (train_idx, test_idx) in enumerate(kfold.split(coords, groups = groups)):\n",
    "        if verbose:\n",
    "            print(f'Fold {i}:')\n",
    "        \n",
    "        train_test_idxs = {'train' : train_idx, 'test' : test_idx}\n",
    "        pp_data = preprocess_data(data, standardize = True, train_test_idxs = train_test_idxs, **pp_args)\n",
    "\n",
    "        # Fitting/predicting differently for direct classification/regression vs. hurdle models\n",
    "        if direct is None:\n",
    "            train_data, test_data = pp_data.iloc[train_idx].copy(deep = True), pp_data.iloc[test_idx].copy(deep = True)\n",
    "\n",
    "            #  clone the model to ensure it fits from scratch... Pymer submodels do this through the wrapper class\n",
    "            #   at fit time and AutoML instances do this when \"keep_search_state\" is False\n",
    "            if sklearn_submodels:\n",
    "                model = clone(model)\n",
    "\n",
    "            #  train the model\n",
    "            with warnings.catch_warnings(action = 'ignore'):\n",
    "                if verbose:\n",
    "                    print('  training model')\n",
    "                model.fit(train_data, fit_args)\n",
    "\n",
    "            #  optionally tuning the probability threshold for the zero component of the hurdle model\n",
    "            if tune_hurdle_thresh:\n",
    "                assert isinstance(model, HurdleModelEstimator), 'Threshold tuning only applies to two-stage hurdle models.'\n",
    "                \n",
    "                X_zero, y_zero, _, _ = get_zero_nonzero_datasets(train_data, extirp_pos = model.extirp_pos, \n",
    "                                                                 pred = False, **model.data_args)\n",
    "                y_pred = model.zero_model.predict_proba(X_zero)[ : , 1]\n",
    "\n",
    "                opt_thresh, _ = test_thresholds(y_pred, y_zero)\n",
    "                model.prob_thresh = round(opt_thresh, 3)\n",
    "\n",
    "                if verbose:\n",
    "                    print(f'  optimal threshold was found to be {model.prob_thresh}')\n",
    "\n",
    "            #  predicting on the test set\n",
    "            y_pred = model.predict(test_data)\n",
    "            y_test = test_data['ratio'].copy(deep = True)\n",
    "\n",
    "            #  back-transforming to go from RRs --> ratios\n",
    "            if back_transform:\n",
    "                y_pred[y_pred != 0] = np.exp(y_pred[y_pred != 0])\n",
    "        else:\n",
    "            assert direct in ['classification', 'regression'], 'The \"direct\" argument must either be \"classification\" or \"regression.\"'\n",
    "\n",
    "            #  getting the data split\n",
    "            X_train, y_train, X_test, y_test = direct_train_test(pp_data, task = direct, already_pp = True, \n",
    "                                                                 train_test_idxs = train_test_idxs)\n",
    "\n",
    "            #  training the model + perform model search\n",
    "            model.fit(X_train = X_train, y_train = y_train, **fit_args)\n",
    "\n",
    "            #  predicting on the test set\n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "        # Get predictions and targets\n",
    "        if verbose:\n",
    "            print('  getting test metrics') \n",
    "\n",
    "        # Discretize ratios for regression models to get classification metrics\n",
    "        #  - case where our predictions are already in the form of DI categories\n",
    "        if direct == 'classification':\n",
    "            true_DI_cats = y_test\n",
    "            pred_DI_cats = y_pred\n",
    "        #  - case where our predictions are in the form of ratios\n",
    "        elif len(class_metrics) != 0:\n",
    "            true_DI_cats = ratios_to_DI_cats(y_test)\n",
    "            pred_DI_cats = ratios_to_DI_cats(y_pred)\n",
    "\n",
    "        # Get regression test metrics for this train/test split\n",
    "        for metric in reg_metrics.keys():\n",
    "            kws = reg_metrics[metric]['kwargs']\n",
    "            metric_dict[metric].append(reg_metrics[metric]['function'](y_test, y_pred, **kws))\n",
    "\n",
    "        # Get per-class classification metrics\n",
    "        for c in classes:\n",
    "            #  binarizing the true/pred labels\n",
    "            true = (true_DI_cats == c).astype(int)\n",
    "            pred = (pred_DI_cats == c).astype(int)\n",
    "\n",
    "            for metric in class_metrics['per_class'].keys():\n",
    "                kws = class_metrics['per_class'][metric]['kwargs']\n",
    "                metric_dict[metric][classes[c]].append(class_metrics['per_class'][metric]['function'](true, pred, **kws))\n",
    "\n",
    "        # Get overall classification metrics\n",
    "        for metric in class_metrics['overall'].keys():\n",
    "            kws = class_metrics['overall'][metric]['kwargs']\n",
    "            metric_dict[metric].append(class_metrics['overall'][metric]['function'](true_DI_cats, pred_DI_cats, **kws))\n",
    "    \n",
    "    return metric_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "aaa82d35-e35b-4abd-b50e-d83320d2277e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_cv_results(metric_dict_sub, class_metrics, reg_metrics, result_type = 'per_class'):\n",
    "\n",
    "    \"\"\"\n",
    "    A helper function to handle the re-formatting of different subsets of the metric dictionary.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    metrics_dict_sub : dictionary\n",
    "    class_metrics : dictionary\n",
    "    reg_metrics : dictionary\n",
    "    results_type : string\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    results : pandas.DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    # Re-structuring per-class classification metrics\n",
    "    if result_type == 'per_class':\n",
    "        metrics = pd.DataFrame(metric_dict_sub)\n",
    "        metrics = pd.concat([metrics[m].explode() for m in class_metrics['per_class']], axis = 1).reset_index()\n",
    "        metrics = metrics.rename(columns = {'index' : 'DI_category'})\n",
    "        metrics = metrics.melt(id_vars = ['DI_category'], value_vars = class_metrics['per_class'], var_name = 'metric')\n",
    "        \n",
    "        results = metrics.groupby(['DI_category', 'metric']).mean()\n",
    "        results = results.rename(columns = {'value' : 'mean'})\n",
    "        results = pd.concat((results, metrics.groupby(['DI_category', 'metric']).std()), axis = 1)\n",
    "        results = results.rename(columns = {'value' : 'std'})\n",
    "    \n",
    "    # Re-structuring overall classification metrics\n",
    "    elif result_type == 'overall':\n",
    "        metrics = pd.DataFrame(metric_dict_sub)\n",
    "        metrics = metrics.melt(id_vars = [], value_vars = class_metrics['overall'], var_name = 'metric')\n",
    "        \n",
    "        results = metrics.groupby('metric').describe()['value'][['mean', 'std']]\n",
    "   \n",
    "    # Re-structuring regression metrics\n",
    "    elif result_type == 'regression':\n",
    "        if 'mean_absolute_error_0-1' in metric_dict_sub:\n",
    "            metric_dict_sub['mean_absolute_error_0-1'] = [mae for mae, _ in metric_dict_sub['mean_absolute_error_0-1']]\n",
    "    \n",
    "        metrics = pd.DataFrame(metric_dict_sub)\n",
    "        metrics = metrics.melt(id_vars = [], value_vars = reg_metrics, var_name = 'metric')\n",
    "    \n",
    "        results = metrics.groupby('metric').describe()['value'][['mean', 'std']]\n",
    "    \n",
    "    # Incorrect input value for result_type arg\n",
    "    else:\n",
    "        raise ValueError('The \"result_type\" argument must be one of \"per_class,\" \"overall,\" or \"regression.\"')\n",
    "\n",
    "    return results\n",
    "\n",
    "def save_cv_results(metrics_dict, model_name, fp, class_metrics = None, reg_metrics = None):\n",
    "\n",
    "    \"\"\"\n",
    "    A helper function to wrap the re-formatting of the results dictionary and saving the resulting\n",
    "    dataframe as a CSV file after cross-validation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    metrics_dict : dictionary\n",
    "    model_name : string\n",
    "    fp : string\n",
    "    class_metrics : dictionary\n",
    "    reg_metrics : dictionary\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    final_results : pandas.DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    # Setting mutable defaults\n",
    "    assert (class_metrics is not None) or (reg_metrics is not None), 'Make sure one of \"class_metrics\" or \"reg_metrics\" is non-empty.'\n",
    "    assert len(metrics_dict) > 0, 'The inputted \"metrics_dict\" has no entries.'\n",
    "    \n",
    "    if class_metrics is None:\n",
    "        class_metrics = {'per_class' : {}, 'overall' : {}}\n",
    "    if reg_metrics is None:\n",
    "        reg_metrics = {}\n",
    "\n",
    "    # Cleaning per-class classification metrics\n",
    "    per_class_dict = {m : metrics_dict[m] for m in class_metrics['per_class']}\n",
    "    if len(per_class_dict) > 0:\n",
    "        per_class_results = format_cv_results(per_class_dict, class_metrics, reg_metrics, result_type = 'per_class')\n",
    "    else:\n",
    "        per_class_results = None\n",
    "\n",
    "    # Cleaning overall classification metrics\n",
    "    overall_dict = {m : metrics_dict[m] for m in class_metrics['overall']}\n",
    "    if len(overall_dict) > 0:\n",
    "        overall_results = format_cv_results(overall_dict, class_metrics, reg_metrics, result_type = 'overall')\n",
    "    else:\n",
    "        overall_results = None\n",
    "\n",
    "    # Cleaning regression metrics\n",
    "    reg_dict = {m : metrics_dict[m] for m in reg_metrics}\n",
    "    if len(reg_dict) > 0:\n",
    "        reg_results = format_cv_results(reg_dict, class_metrics, reg_metrics, result_type = 'regression')\n",
    "    else:\n",
    "        reg_results = None\n",
    "\n",
    "    # Merging the cleaned results dataframes\n",
    "    columns = ['DI_category', 'metric', 'mean', 'std']\n",
    "    final_results = pd.DataFrame({c : [np.nan] for c in columns}) # empty dataframe to add on to\n",
    "    \n",
    "    if per_class_results is not None: \n",
    "        final_results = pd.concat((final_results, per_class_results.reset_index()))\n",
    "    if overall_results is not None:\n",
    "        final_results = pd.concat((final_results, overall_results.reset_index()))\n",
    "    if reg_results is not None:\n",
    "        final_results = pd.concat((final_results, reg_results.reset_index()))\n",
    "        \n",
    "    final_results = final_results.reset_index(drop = True).dropna(axis = 0, how = 'all')\n",
    "    \n",
    "    #  adding on some last few bits of information\n",
    "    final_results = final_results.rename(columns = {'std' : 'standard_deviation'})\n",
    "    final_results['DI_category'] = final_results['DI_category'].fillna('overall')\n",
    "    final_results['date'] = datetime.today().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    final_results['model_name'] = model_name\n",
    "\n",
    "    # Saving to the inputted file\n",
    "    if os.path.isfile(fp):\n",
    "        existing_results = pd.read_csv(fp)\n",
    "        all_results = pd.concat((existing_results, final_results))\n",
    "        all_results.to_csv(fp, index = False)\n",
    "    else:\n",
    "        final_results.to_csv(fp, index = False)\n",
    "\n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813612ee-39c9-4bf4-b214-7ad540553ca3",
   "metadata": {},
   "source": [
    "# Actually running the cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c167098-8e47-48b4-a776-0f134c0040ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the metrics to use\n",
    "class_metrics = {'per_class' : {'balanced_accuracy' : {'function' : balanced_accuracy_score,\n",
    "                                                       'kwargs' : {}\n",
    "                                                      },\n",
    "                                'sensitivity' : {'function' : recall_score,\n",
    "                                                 'kwargs' : {'pos_label' : 1}\n",
    "                                                 },\n",
    "                                'specificity' : {'function' : recall_score,\n",
    "                                                 'kwargs' : {'pos_label' : 0}\n",
    "                                                 }\n",
    "                               },\n",
    "                  'overall' : {'balanced_accuracy_overall' : {'function' : balanced_accuracy_score,\n",
    "                                                              'kwargs' : {}\n",
    "                                                             }\n",
    "                              }\n",
    "                }\n",
    "reg_metrics = {'mean_absolute_error' : {'function' : mean_absolute_error,\n",
    "                                        'kwargs' : {}\n",
    "                                       },\n",
    "               'mean_absolute_error_0-1' : {'function' : mean_absolute_error_range,\n",
    "                                            'kwargs' : {'lower_bound' : 0,\n",
    "                                                        'upper_bound' : 1\n",
    "                                                       }\n",
    "                                           }\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "addb80a9-65a5-498e-b725-93d74e0aa740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing the model to use, among pymer, sklearn, FLAML_hurdle, FLAML_regression, FLAML_classification\n",
    "model_to_use = 'pymer'\n",
    "\n",
    "#  pymer hurdle model, for sanity checking\n",
    "if model_to_use == 'pymer':\n",
    "    formula_zero = 'local_extirpation ~ BM + DistKm + I(DistKm^2) + PopDens + Stunting + Reserve + (1|Country) + (1|Species) + (1|Study)'\n",
    "    formula_nonzero = 'RR ~ BM + DistKm + I(DistKm^2) + PopDens + I(PopDens^2) + BM*DistKm + (1|Country) + (1|Species) + (1|Study)'\n",
    "    control_str = \"optimizer='bobyqa', optCtrl=list(maxfun=1e5)\"\n",
    "    \n",
    "    use_rfx = True\n",
    "    extirp_pos = False\n",
    "    \n",
    "    zero_model = PymerModelWrapper(Lmer, formula = formula_zero, family = 'binomial', control_str = control_str, \n",
    "                                   use_rfx = use_rfx)\n",
    "    nonzero_model = PymerModelWrapper(Lmer, formula = formula_nonzero, family = 'gaussian', use_rfx = use_rfx)\n",
    "    model = HurdleModelEstimator(zero_model, nonzero_model, extirp_pos = extirp_pos, \n",
    "                                 data_args = {'outlier_cutoff' : 15})\n",
    "\n",
    "    back_transform = True\n",
    "    sklearn_submodels = False\n",
    "    direct = None\n",
    "    tune_hurdle_thresh = True\n",
    "    \n",
    "    fit_args = None\n",
    "    pp_args = {'include_indicators' : False,\n",
    "               'include_categorical' : True,\n",
    "               'polynomial_features' : 0,\n",
    "               'log_trans_cont' : True}\n",
    "#  a version of the sklearn model used\n",
    "elif model_to_use == 'sklearn':\n",
    "    extirp_pos = False\n",
    "    verbose = False\n",
    "\n",
    "    grid_cv = 5\n",
    "    logistic_penalty = 'l1'\n",
    "\n",
    "    indicator_columns = ['Diet']\n",
    "    nonzero_columns = ['BM', 'DistKm', 'PopDens', 'Stunting', 'TravTime', 'LivestockBio', 'Literacy', 'Reserve']\n",
    "    zero_columns = nonzero_columns\n",
    "    data_args = {'indicator_columns' : indicator_columns,\n",
    "                 'nonzero_columns' : nonzero_columns,\n",
    "                 'zero_columns' : zero_columns}\n",
    "\n",
    "    #  hyperparameters for grid search - reduced for testing purposes!\n",
    "    l1_ratio = [0.1, 0.5, 1]\n",
    "    Cs = 5\n",
    "\n",
    "    nonzero_model = ElasticNetCV(cv = grid_cv, l1_ratio = l1_ratio, max_iter = 5000)\n",
    "    zero_model = LogisticRegressionCV(cv = grid_cv, Cs = Cs, penalty = logistic_penalty, solver = 'saga', max_iter = 500)\n",
    "    model = HurdleModelEstimator(zero_model, nonzero_model, extirp_pos = extirp_pos, verbose = verbose,\n",
    "                                 data_args = data_args)\n",
    "    \n",
    "    back_transform = True\n",
    "    sklearn_submodels = True\n",
    "    direct = None\n",
    "    tune_hurdle_thresh = True\n",
    "    \n",
    "    fit_args = None\n",
    "    pp_args = {'include_indicators' : False,\n",
    "               'include_categorical' : False,\n",
    "               'polynomial_features' : 2,\n",
    "               'log_trans_cont' : False}\n",
    "elif model_to_use == 'FLAML_hurdle':\n",
    "    time_budget_mins = 0.1\n",
    "    base_path = os.path.join('..', 'model_saves')\n",
    "    verbose = 0\n",
    "    extirp_pos = False\n",
    "    \n",
    "    zero_columns = ['BM', 'DistKm', 'PopDens', 'Stunting', 'TravTime', 'LivestockBio', 'Literacy', 'Reserve']\n",
    "    nonzero_columns = zero_columns\n",
    "    indicator_columns = []\n",
    "    \n",
    "    zero_metric = balanced_accuracy_FLAML\n",
    "    nonzero_metric = 'mse'\n",
    "    \n",
    "    # Setting up the zero and nonzero models\n",
    "    zero_model = AutoML()\n",
    "    nonzero_model = AutoML()\n",
    "    \n",
    "    #  specify fitting paramaters\n",
    "    zero_settings = {\n",
    "        'time_budget' : time_budget_mins * 60,  # in seconds\n",
    "        'metric' : zero_metric,\n",
    "        'task' : 'classification',\n",
    "        'log_file_name' : os.path.join(base_path, f'nonlinear_hurdle.log'),\n",
    "        'seed' : 1693,\n",
    "        'estimator_list' : ['lgbm', 'xgboost', 'xgb_limitdepth', 'rf', \n",
    "                            'extra_tree', 'kneighbor', 'lrl1', 'lrl2'],\n",
    "        'early_stop' : True,\n",
    "        'verbose' : verbose,\n",
    "        'keep_search_state' : True\n",
    "    }\n",
    "    \n",
    "    nonzero_settings = {\n",
    "        'time_budget' : time_budget_mins * 60,  # in seconds\n",
    "        'metric' : nonzero_metric,\n",
    "        'task' : 'regression',\n",
    "        'log_file_name' : os.path.join(base_path, f'nonlinear_hurdle.log'),\n",
    "        'seed' : 1693,\n",
    "        'estimator_list' : ['lgbm', 'xgboost', 'xgb_limitdepth', 'rf', 'extra_tree', 'kneighbor'],\n",
    "        'early_stop' : True,\n",
    "        'verbose' : verbose,\n",
    "        'keep_search_state' : True\n",
    "    }\n",
    "    \n",
    "    #  dumping everything into the hurdle model wrapper\n",
    "    data_args = {'indicator_columns' : indicator_columns,\n",
    "                 'nonzero_columns' : nonzero_columns,\n",
    "                 'zero_columns' : zero_columns}\n",
    "    model = HurdleModelEstimator(zero_model, nonzero_model, extirp_pos = extirp_pos, \n",
    "                                 data_args = data_args, verbose = False)\n",
    "\n",
    "    back_transform = True\n",
    "    sklearn_submodels = False\n",
    "    direct = None\n",
    "    tune_hurdle_thresh = True\n",
    "    \n",
    "    fit_args = {'zero' : zero_settings, 'nonzero' : nonzero_settings}\n",
    "    pp_args = {'include_indicators' : False,\n",
    "               'include_categorical' : False,\n",
    "               'polynomial_features' : 0,\n",
    "               'log_trans_cont' : False}\n",
    "elif model_to_use == 'FLAML_regression':\n",
    "    # Initialize the auto ML instance\n",
    "    model = AutoML()\n",
    "    time_budget_mins = 0.1\n",
    "    \n",
    "    # Specify paramaters\n",
    "    base_path = os.path.join('..', 'model_saves', f'direct_regression')\n",
    "    automl_settings = {\n",
    "        'time_budget' : time_budget_mins * 60,  # in seconds\n",
    "        'metric' : 'mse',\n",
    "        'task' : 'regression',\n",
    "        'log_file_name' : os.path.join(base_path, f'mammals_direct_regression.log'),\n",
    "        'seed' : 1693,\n",
    "        'estimator_list' : ['lgbm', 'xgboost', 'xgb_limitdepth', 'rf', 'extra_tree', 'kneighbor'],\n",
    "        'early_stop' : True,\n",
    "        'verbose' : 0\n",
    "    }\n",
    "\n",
    "    back_transform = False\n",
    "    sklearn_submodels = False\n",
    "\n",
    "    fit_args = automl_settings\n",
    "    pp_args = {'include_indicators' : False,\n",
    "               'include_categorical' : False,\n",
    "               'polynomial_features' : 0,\n",
    "               'log_trans_cont' : False}\n",
    "    direct = 'regression'\n",
    "elif model_to_use == 'FLAML_classification':\n",
    "    # Initialize the auto ML instance\n",
    "    model = AutoML()\n",
    "    time_budget_mins = 0.1\n",
    "    \n",
    "    # Specify paramaters\n",
    "    base_path = os.path.join('..', 'model_saves', f'direct_regression')\n",
    "    automl_settings = {\n",
    "        'time_budget' : time_budget_mins * 60,  # in seconds\n",
    "        'metric' : balanced_accuracy_FLAML,\n",
    "        'task' : 'classification',\n",
    "        'log_file_name' : os.path.join(base_path, f'mammals_direct_regression.log'),\n",
    "        'seed' : 1693,\n",
    "        'estimator_list' : ['lgbm', 'xgboost', 'xgb_limitdepth', 'rf', 'extra_tree', 'kneighbor'],\n",
    "        'early_stop' : True,\n",
    "        'verbose' : 0\n",
    "    }\n",
    "\n",
    "    back_transform = False\n",
    "    sklearn_submodels = False\n",
    "\n",
    "    fit_args = automl_settings\n",
    "    pp_args = {'include_indicators' : False,\n",
    "               'include_categorical' : False,\n",
    "               'polynomial_features' : 0,\n",
    "               'log_trans_cont' : False}\n",
    "    reg_metrics = None\n",
    "    direct = 'classification'\n",
    "    \n",
    "# Cross-validation specific params\n",
    "num_folds = 5\n",
    "block_type = None\n",
    "group_col = None\n",
    "spatial_spacing = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca63aa47-c1a6-415c-92ff-47b3c22f98a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pymer\n",
      "Using standard cross-validation\n",
      "Fold 0:\n",
      "  training model\n",
      "  optimal threshold was found to be 0.85\n",
      "  getting test metrics\n",
      "Fold 1:\n",
      "  training model\n",
      "  optimal threshold was found to be 0.85\n",
      "  getting test metrics\n",
      "Fold 2:\n",
      "  training model\n",
      "  optimal threshold was found to be 0.8\n",
      "  getting test metrics\n",
      "Fold 3:\n",
      "  training model\n",
      "  optimal threshold was found to be 0.85\n",
      "  getting test metrics\n",
      "Fold 4:\n",
      "  training model\n",
      "  optimal threshold was found to be 0.8\n",
      "  getting test metrics\n"
     ]
    }
   ],
   "source": [
    "# Run the cross-validation using the inputted params\n",
    "print(f'Using {model_to_use}')\n",
    "metrics_dict = run_cross_val(model, ben_lop2019, block_type = block_type, num_folds = num_folds, \n",
    "                             group_col = group_col, spatial_spacing = spatial_spacing, fit_args = fit_args, \n",
    "                             pp_args = pp_args, class_metrics = class_metrics, reg_metrics = reg_metrics, \n",
    "                             verbose = True, random_state = 1693, sklearn_submodels = sklearn_submodels, \n",
    "                             back_transform = back_transform, direct = direct, tune_hurdle_thresh = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f44bdd0-63a9-40de-b560-ad65eb6ad77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = '/Users/emiliolr/Desktop/test_cv_results.csv'\n",
    "save_cv_results(metrics_dict, 'Model Name', fp, class_metrics, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c5050f-3729-465c-8588-459f2fa92e10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
